{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c22ae5-ed1d-4015-aafc-bafdeec71bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this lab, you will be using the [Sakila](https://dev.mysql.com/doc/sakila/en/) database of movie rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da0955-ea51-44b4-a103-f55dade56354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to optimize our inventory, we would like to predict if a film will have more monthly rentals in July than in June. Create a model to predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598b1d8a-388d-4477-a477-6568620b85ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import getpass  # To get the password without showing the input\n",
    "password = getpass.getpass() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac50c96-3aaa-4d82-9e6e-d0cfda8099e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270deaf1-d7d0-4fc5-8dbe-70f4c24be2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62750b27-8099-4367-946d-cb2ebf3780e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create a query or queries to extract the information you think may be relevant for building the prediction model. It should include some film features and some rental features. Use the data from 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5473e2dc-f167-4efc-8468-b5e5629eb06e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OptionEngine' object has no attribute 'execute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      2\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(connection_string)\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m SELECT f.film_id, f.title, f.length, f.rental_rate, f.release_year, f_a.actor_id, CONCAT(a.first_name, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,a.last_name) as actor_name, f_c.category_id, c.name as category\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mFROM film as f \u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mJOIN film_category as f_c \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mON f_a.actor_id = a.actor_id\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mORDER BY f.film_id ASC \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 16\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OptionEngine' object has no attribute 'execute'"
     ]
    }
   ],
   "source": [
    "connection_string = 'mysql+pymysql://root:' + password + '@localhost/sakila'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "query = ''' SELECT f.film_id, f.title, f.length, f.rental_rate, f.release_year, f_a.actor_id, CONCAT(a.first_name, \" \",a.last_name) as actor_name, f_c.category_id, c.name as category\n",
    "FROM film as f \n",
    "JOIN film_category as f_c \n",
    "ON f.film_id = f_c.film_id \n",
    "JOIN category as c \n",
    "ON f_c.category_id = c.category_id \n",
    "JOIN film_actor as f_a \n",
    "ON f.film_id = f_a.film_id \n",
    "JOIN actor as a \n",
    "ON f_a.actor_id = a.actor_id\n",
    "ORDER BY f.film_id ASC '''\n",
    "\n",
    "data = pd.read_sql_query(query, engine)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef379a-b726-424a-9b93-447cdc4de2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Create a query to get the total amount of rentals in June for each film. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683755d7-db35-45ad-b89b-a6461b355e45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OptionEngine' object has no attribute 'execute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT f.film_id, f.title, COUNT(p.rental_id) as num_of_rentals\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mFROM film as f\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mORDER BY COUNT(p.rental_id) DESC;\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 15\u001b[0m data2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m data2\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OptionEngine' object has no attribute 'execute'"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT f.film_id, f.title, COUNT(p.rental_id) as num_of_rentals\n",
    "FROM film as f\n",
    "JOIN inventory as i \n",
    "ON f.film_id = i.film_id \n",
    "JOIN rental as r \n",
    "ON i.inventory_id = r.inventory_id \n",
    "JOIN payment as p \n",
    "ON r.rental_id = p.rental_id\n",
    "WHERE (rental_date BETWEEN \"2005-06-01 00:00:00\" AND \"2005-06-30 23:59:59\")\n",
    "GROUP BY f.film_id\n",
    "ORDER BY COUNT(p.rental_id) DESC;\n",
    "'''\n",
    "\n",
    "data2 = pd.read_sql_query(query, engine)\n",
    "data2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd45970-4c25-4b61-a1a5-8f603c184e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Do the same with July. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a125fde3-52be-4a48-b39e-9a15c2c2e63f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OptionEngine' object has no attribute 'execute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT f.film_id, f.title, COUNT(p.rental_id) as num_of_rentals\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mFROM film as f\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mORDER BY COUNT(p.rental_id) DESC;\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 15\u001b[0m data3 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m data3\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OptionEngine' object has no attribute 'execute'"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT f.film_id, f.title, COUNT(p.rental_id) as num_of_rentals\n",
    "FROM film as f\n",
    "JOIN inventory as i \n",
    "ON f.film_id = i.film_id \n",
    "JOIN rental as r \n",
    "ON i.inventory_id = r.inventory_id \n",
    "JOIN payment as p \n",
    "ON r.rental_id = p.rental_id\n",
    "WHERE (rental_date BETWEEN \"2005-07-01 00:00:00\" AND \"2005-07-31 23:59:59\")\n",
    "GROUP BY f.film_id\n",
    "ORDER BY COUNT(p.rental_id) DESC;\n",
    "'''\n",
    "\n",
    "data3 = pd.read_sql_query(query, engine)\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf6cee-181e-473f-b03f-d17e7732213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Create a new column containing (Yes/No) for each film whether or not the number of monthly rentals in **July was bigger than in June**. Your objective will be to predict this new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f6f76-cd5f-4991-8950-3a1ed285d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''' \n",
    "SELECT f.film_id, f.title, \n",
    "       COUNT(DISTINCT p.rental_id) as num_of_rentals,\n",
    "       (CASE WHEN \n",
    "           (SUM(CASE WHEN rental_date BETWEEN \"2005-07-01 00:00:00\" AND \"2005-07-31 23:59:59\" THEN 1 ELSE 0 END)\n",
    "            > \n",
    "            SUM(CASE WHEN rental_date BETWEEN \"2005-06-01 00:00:00\" AND \"2005-06-30 23:59:59\" THEN 1 ELSE 0 END)) \n",
    "        THEN \"Yes\" ELSE \"No\" END) as more_rentals_in_july_than_june\n",
    "FROM film as f\n",
    "JOIN inventory as i\n",
    "ON f.film_id = i.film_id \n",
    "JOIN rental as r\n",
    "ON i.inventory_id = r.inventory_id \n",
    "JOIN payment as p\n",
    "ON r.rental_id = p.rental_id \n",
    "GROUP BY f.film_id, f.title\n",
    "ORDER BY SUM(CASE WHEN rental_date BETWEEN \"2005-07-01 00:00:00\" AND \"2005-07-31 23:59:59\" THEN 1 ELSE 0 END) DESC;\n",
    "'''\n",
    "\n",
    "data4 = pd.read_sql_query(query, engine) \n",
    "data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d017d-1eb4-48be-8b33-18027a752548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Read the data into a Pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37816dc7-28d3-4d42-bf17-e5f5f895ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql('SELECT * FROM june_july_predictions', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfbcd79-52bb-4415-bc09-b450cbafb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df1) \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649e3c3-0a8c-4a97-a71c-5e47b416716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Analyze extracted features and transform them. You may need to encode some categorical variables or scale numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b770abc-91aa-419a-ada7-b2fe3bc530ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include = np.number)\n",
    "df_categorical = df.select_dtypes(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06105e-e419-4cc9-810a-25c0a228b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f624238-ec90-45ef-b3b1-db54241033d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc67034-626d-4b6d-8eac-9657df26a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7edf2-f729-4422-aab3-07ba54c530cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13654ca-745d-4b80-81aa-338ebf662c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(df_categorical)\n",
    "\n",
    "categorical_encoded = encoder.transform(df_categorical).toarray()\n",
    "\n",
    "df_categorical_encoded = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad255e2a-02bc-4824-87fd-e185e323e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df_numeric, df_categorical_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3f730-6eae-4514-a8f6-1f80b5914a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Create a logistic regression model to predict this new column from the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972da63-7128-4d5e-aaf3-2b245e5d80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order of Operations- \n",
    "# 1. X-y split- isolate the column we want to predict\n",
    "# 2. Train-test split- establish X_train/test, y_train/test\n",
    "# 3. Introduce a scaler to ensure all of our numerical variables are on the same scale- use MinMax because we're using the encoder next\n",
    "# 4. Introduce a OneHot Encoder to encode all of the categorical variables  \n",
    "# 5. Concat our encoded with the scaled numerical variables produced in Step 3 \n",
    "# 6. Use model.fit() to \"train\" the model \n",
    "\n",
    "# 7. For the numericals, do logistic regression- lr = LogisticRegression(random_state = 0, solver = 'lbfgs'), lr.fit(X_train_scaled, y_train)\n",
    "# 8. Compute the r2score, mae, mse, rmse for the numericals as well\n",
    "# 8. Start getting probabilities, apply lr.predict_proba(X_train_scaled).sum(axis = 1), apply later lr.predict(X_train_scaled) \n",
    "# 9. Compute the logarithim- logits \n",
    "# 10. Display and visualise using sns.scatterplot(y = logits, x = X_train_scaled['Column']); plt.show() \n",
    "# 11. For the categoricals, calculate accuracy, precision, recall, F1, Cohen-Kappa \n",
    "# 12. Generate a confusion matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b49f3-8d8c-4a4c-b433-daf24048528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. X-y split- isolate the column we want to predict- I'm not sure which we're supposed to predict from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567a17b-bf03-4c53-b457-9e90633fa0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df['in_the_top_20_actors']\n",
    "X = new_df.drop(['in_the_top_20_actors'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da49d7a-a476-44c0-84f2-488346172642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train-test split- establish X_train/test, y_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d51bef-d69d-46e1-9ae8-d9f9a4cbacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08fbe05-c55c-48ef-b31c-70af0a422377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Introduce a scaler to ensure all of our numerical variables are on the same scale- use MinMax because we're using the encoder next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fe5b8-f0ea-46e9-963b-03f2452fb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train_num = X_train.select_dtypes(include = np.number)\n",
    "X_test_num = X_test.select_dtypes(include = np.number)\n",
    "\n",
    "transformer = MinMaxScaler().fit(X_train_num) \n",
    "X_train_normalized = transformer.transform(X_train_num)\n",
    "X_test_normalized = transformer.transform(X_test_num)\n",
    "\n",
    "X_train_norm = pd.DataFrame(X_train_normalized, columns = X_train_num.columns)\n",
    "X_test_norm = pd.DataFrame(X_test_normalized, columns = X_test_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1111ea-a1d0-48e5-98e9-a707cfd32cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm.columns = X_train_num.columns\n",
    "X_train_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b819f1-470c-4142-8f96-923c8b058158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Concat our encoded with the scaled numerical variables produced in Step 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc625c-2c46-40b8-93e4-944187c432f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_encoded = pd.concat([X_train_norm, df_categorical_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87646a-bc5b-4419-8faa-ed73093150d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Use model.fit() to \"train\" the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a39ab-2d81-411a-aed4-a769c3ead86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = df_scaled_encoded()\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876ec99-3185-450e-b6dc-05addaa37bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. For the numericals, do logistic regression- lr = LogisticRegression(random_state = 0, solver = 'lbfgs'), lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b0684-9cbb-452c-ac3b-54be42bf3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bebfa-c0bd-46a0-a51b-f2d935f52dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 0, solver = 'lbfgs') \n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990d2e1-94b0-431b-83f7-b85debe266fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Compute the r2score, mae, mse, rmse for the numericals as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b0fff-1631-4aae-954d-0e94362f54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, mean_squared_error\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d118f-0d69-45e6-b83c-61ba239959e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Start getting probabilities, apply lr.predict_proba(X_train_scaled).sum(axis = 1), apply later lr.predict(X_train_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf364df-b6f5-4ce1-bd17-09d52907e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023f591-6e1b-4906-ad6e-eb266b3b69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classification.predict(X_test_transformed)\n",
    "classification.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39419393-e8b3-4ee3-9124-f08cf18fbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. For the categoricals, calculate accuracy, precision, recall, F1, Cohen-Kappa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7aee8-57ff-4a7b-94ac-c01f7743c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32c2ae-7193-47e3-be27-df2d753f3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Generate a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe177-3f84-4d4e-899a-5fd2dbd448c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
